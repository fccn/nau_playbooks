---
# Ansible playbook that deploys NAU.
#
#
# Open edX instance with 3 layers:
# - Load balancer servers
# - Persistence servers
# - Application servers (only partially)
#
# NAU also has other services like:
# - richie for marketing site
# - static proxy service
# - fam servers that print course certificate to PDF
#
# Examples:
#   To deploy the application servers and deploy 2 servers at once run:
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit app_servers -e serial_number=2
#
#   To deploy the application servers and migrate the database changes
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit app_servers -e "migrate_db=yes"
#
#   To deploy the application servers with a specific themes version
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit app_servers -e "THEMES_VERSION=master"
#
#   To deploy the load balancers run:
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit balancer_servers
#
#   To deploy IdP proxy servers run:
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit idpproxy_servers
#
#   To mark a server for maintenance, like load balancer or mysql server, append to the shell command arguments:
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --tags keepalived --limit redis_docker_servers[0] -e keepalived_priority=1
#
#   To mark a server back to normal state, run the same command without the --limit.
#
#   To deploy the docker registry mirror
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit registry_mirror_server -e registrymirror_deploy=true
#
#   To deploy Mongo DB
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit mongo_docker_servers -e mongo_deploy=true
#
#   To deploy Redis cache
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit redis_docker_servers -e redis_deploy=true
#
#   To deploy ElasticSearch cluster
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit elasticsearch_docker_servers -e elasticsearch_deploy=true
#
#   To deploy Richie MySQL
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit richie_mysql_docker_servers -e richie_mysql_deploy=true
#
#   To deploy Observability
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit observability_docker_servers -e observability_docker_deploy=true
#
#   To deploy Financial Manager MySQL database
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit financial_manager_mysql_docker_servers -e financial_manager_mysql_deploy=true
#
#   To deploy Financial Manager application
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit financial_manager_docker_servers -e financial_manager_deploy=true
#
#   To deploy NAU Analytics MySQL database
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit nau_analytics_mysql_docker_servers -e nau_analytics_mysql_deploy=true
#
#   To deploy Percona XtraDB databases
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit xtradb_servers -e xtradb_deploy=true
#
#   To deploy ClickHouse databases
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit clickhouse_servers -e clickhouse_deploy=true
#
#   To deploy Kubernetes nodes
#     ansible-playbook -i nau-data/envs/<env>/hosts.ini deploy.yml --limit kubernetes_servers -e kubernetes_deploy=true
#

- name: Infrastructure configuration
  hosts: all
  become: True
  gather_facts: True
  tasks:
    - import_role:
        name: users
      when: ansible_distribution == 'Ubuntu'
    - import_role:
        name: ssh_authorized_keys

    - name: On Ubuntu 20.04 or later, you need to install the acl package to becoming non privileged user.
      # Fix of this error:
      #   Failed to set permissions on the temporary files Ansible needs to create when becoming an unprivileged user 
      #   (rc: 1, err: chown: changing ownership of  Operation not permitted
      #   https://github.com/georchestra/ansible/issues/55
      package:
        name: acl
      # need this package so pakiti is installed correctly on NAU
      tags: pakiti
      when: ansible_distribution == 'Ubuntu' and (ansible_distribution_major_version|int) >= 20

    - import_role:
        name: ansible_pakiti_client

    # - name: Install pip
    #   import_role:
    #     name: geerlingguy.pip
    #   when: ansible_distribution == 'Ubuntu' and (ansible_distribution_major_version|int) >= 20

    - name: Configure sysconfig custom values
      sysctl:
        name: "{{ sysconfig_config.name }}"
        value: "{{ sysconfig_config.value }}"
        state: "{{ sysconfig_config.state | default(omit) }}"
        reload: "{{ sysconfig_config.reload | default(omit) }}"
        sysctl_set: "{{ sysconfig_config.sysctl_set | default(omit) }}"
      loop: "{{ sysconfig_configuration | default({}) }}"
      loop_control:
        loop_var: sysconfig_config
        label: "{{ sysconfig_config.name }}"
      tags: sysctl

- name: Bootstrap cnc with Ubuntu 20.04
  hosts: command_and_control_2004
  become: True
  gather_facts: True
  tasks:
    # to run openedx-lms-db-export.yml playbook that runs a script on the a docker container
    - name: Install docker
      import_role:
        name: geerlingguy.docker
      when: ansible_distribution == 'Ubuntu' and (ansible_distribution_major_version|int) >= 20

- name: Configure hosts file
  hosts: all,!support_server,!command_and_control
  become: True
  gather_facts: True
  # vars defined on 02_hosts.yml
  roles: 
    - bertvv.hosts
  tags: hosts

- name: Configure load balancer docker
  # Deploy before on backup balancer, so if there is an error during the ansible deployment the primary continue to serve
  hosts: balancer_servers
  serial: "{{ serial_number | default(1) }}"
  become: True
  gather_facts: True
  roles:
    - role: keepalived
      vars: 
        keepalived_priority_override: 1 # Lower priority so the VIPs can swap to other machine.
      when: balancer_keepalived_vrrp_instances is defined
    - role: geerlingguy.docker
    - role: ansible-firewall
    - role: ansible-docker-deploy
    - role: haproxy-netsnmp-perl
    - role: snmpd
    - role: nau_check_urls
    - role: keepalived
      vars:
        keepalived_priority_override: ""
      when: balancer_keepalived_vrrp_instances is defined
  tags: load-balancer

# specific variables are configured on nau-data/envs/<env>/group_vars/idpproxy_servers.yml
- name: Configure Shibboleth IdP
  hosts: 
    - idpproxy_servers
  become: True
  gather_facts: True
  tags: 
    - idp
  roles:
    - role: server_files
      vars: 
        server_files: "{{ server_files_before }}"
    - shibboleth-3-4-4-centos7-lite/roles/check-system
    # - shibboleth-3-4-4-centos7-lite/roles/system
    - shibboleth-3-4-4-centos7-lite/roles/httpd
    - shibboleth-3-4-4-centos7-lite/roles/tomcat
    - shibboleth-3-4-4-centos7-lite/roles/postgresql
    - shibboleth-3-4-4-centos7-lite/roles/shibboleth
    - shibboleth-3-4-4-centos7-lite/roles/shibboleth_files
    - role: shibboleth-3-4-4-centos7-lite/roles/cc-cmd
      when: install_cc_cmd is not defined or install_cc_cmd
    - shibboleth-3-4-4-centos7-lite/roles/performance
    - role: server_files
      vars: 
        server_files: "{{ server_files_after }}"

- name: Configure support server
  hosts: support_server
  become: True
  gather_facts: True
  roles:
    - role: geerlingguy.docker
    - role: openark_orchestrator


- name: Configure docker registry mirror
  hosts: registry_mirror_server
  become: True
  gather_facts: True
  vars:
    _registry_mirror_deploy : "{{ registrymirror_deploy | default(false) | bool }}"
  tasks:
    - name: Deploy registry mirror
      import_role:
        name: registry_mirror_deploy
      when: _registry_mirror_deploy

- name: Deploy Mongo servers
  hosts: mongo_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: mongo_deploy | default(false) | bool

    - import_role:
        name: snmpd
      when: mongo_deploy | default(false) | bool
    - import_role:
        name: snmpd_docker
      when: mongo_deploy | default(false) | bool

    - name: Deploy mongo
      import_role:
        name: mongo_docker_deploy
      vars:
        # mongo_docker_instance_name
        mongo_docker_initdb_root_username: "{{ MONGO_ADMIN_USER | default('admin') if (mongo_docker_init | default(false) | bool) else '' }}"
        mongo_docker_initdb_root_password: "{{ MONGO_ADMIN_PASSWORD if (mongo_docker_init | default(false) | bool) else '' }}"
        mongo_docker_replSet: "{{ MONGO_REPL_SET }}"
        mongo_docker_keyFile_value: "{{ MONGO_CLUSTER_KEY | default('') }}"
        mongo_docker_command_bind_ip_additional_list:
          - "{{ ansible_additional_ipv4 | default('') }}"
      when: mongo_deploy | default(false) | bool

- name: Set Mongo Feature compatibility
  hosts: mongo_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Get Primary MongoDB Node
      shell: |
        make --no-print-directory --directory {{ mongo_docker_deploy_base_folder }} mongo-primary-node
      register: get_primary_mongodb_node_output
      when: mongo_feature_compatibility_version | default(False)

    - name: Set Mongo Feature compatibility
      shell: |
        docker exec {{ mongo_docker_container_name }} {{ mongo_shell_command }} --username {{ mongo_docker_admin_username }} --password {{ mongo_docker_admin_password }} --eval "db.adminCommand( { setFeatureCompatibilityVersion: '{{ mongo_feature_compatibility_version }}'{% if mongo_feature_compatibility_confirm | default(False) %}, confirm: true{% endif %} } )"
      delegate_to: "{{ get_primary_mongodb_node_output is defined and get_primary_mongodb_node_output.stdout }}"
      run_once: true
      when: mongo_feature_compatibility_version | default(False)

    - name: Get Mongo Feature compatibility
      shell: |
        docker exec {{ mongo_docker_container_name }} {{ mongo_shell_command }} --username {{ mongo_docker_admin_username }} --password {{ mongo_docker_admin_password }} --eval "JSON.stringify(db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } ))" | tail -n1 | jq -r '.featureCompatibilityVersion.version'
      register: get_mongo_feature_compatibility_version_output
      delegate_to: "{{ get_primary_mongodb_node_output is defined and get_primary_mongodb_node_output.stdout }}"
      run_once: true
      when: mongo_feature_compatibility_version | default(False)

    - name: Check Mongo Feature compatibility
      fail:
        msg: Incorrect Mongo feature compatibility version, current '{{ get_mongo_feature_compatibility_version_output.stdout }}' expected '{{ mongo_feature_compatibility_version }}'
      delegate_to: "{{ get_primary_mongodb_node_output.stdout }}"
      run_once: true
      when: mongo_feature_compatibility_version | default(False) and get_mongo_feature_compatibility_version_output.stdout != mongo_feature_compatibility_version

    - import_tasks: tasks/healthcheck.yml
      when: mongo_feature_compatibility_version | default(False)

- name: Deploy Elasticsearch servers
  hosts: elasticsearch_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: elasticsearch_deploy | default(false) | bool

    - import_role:
        name: snmpd
      when: elasticsearch_deploy | default(false) | bool
    - import_role:
        name: snmpd_docker
      when: elasticsearch_deploy | default(false) | bool

    - name: Deploy elasticsearch
      import_role:
        name: elasticsearch_docker_deploy
      vars:
        elasticsearch_network_hosts_additional_list:
          - "{{ ansible_additional_ipv4 | default('') }}"
      when: elasticsearch_deploy | default(false) | bool

- name: Deploy cache redis docker servers
  hosts: redis_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Lower keepalived priority to force VIP swap
      import_role:
        name: keepalived
      vars: 
        keepalived_priority_override: 1 # Lower priority so the VIPs can swap to other machine.
      tags: keepalived
      when: ( redis_deploy | default(false) | bool ) and redis_keepalived_vrrp_instances is defined and ( groups['redis_docker_servers'] | length ) > 1

    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: redis_deploy | default(false) | bool

    - import_role:
        name: snmpd
      tags: snmpd
      when: redis_deploy | default(false) | bool
    - import_role:
        name: snmpd_docker
      tags: snmpd
      when: redis_deploy | default(false) | bool

    - name: Deploy cache redis
      import_role:
        name: redis_docker_deploy
      when: redis_deploy | default(false) | bool

    - name: Restore keepalived priority
      import_role:
        name: keepalived
      vars:
        keepalived_priority_override: ""
      tags: keepalived
      when: ( redis_deploy | default(false) | bool ) and redis_keepalived_vrrp_instances is defined and ( groups['redis_docker_servers'] | length ) > 1

- name: Deploy richie mysql docker servers
  hosts: richie_mysql_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Lower keepalived priority to force VIP swap
      import_role:
        name: keepalived
      vars: 
        keepalived_priority_override: 1 # Lower priority so the VIPs can swap to other machine.
      tags: keepalived
      when: ( richie_mysql_deploy | default(false) | bool ) and richie_mysql_keepalived_vrrp_instances is defined and ( groups['richie_mysql_docker_servers'] | length ) > 1

    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: richie_mysql_deploy | default(false) | bool

    - import_role:
        name: snmpd
      tags: snmpd
      when: richie_mysql_deploy | default(false) | bool
    - import_role:
        name: snmpd_docker
      tags: snmpd
      when: richie_mysql_deploy | default(false) | bool

    - name: Deploy richie mysql
      import_role:
        name: mysql_docker_deploy
      vars:
        # Richie shared variables to all environments
        richie_nau_MYSQL_DATABASE: richie_nau
        richie_nau_MYSQL_USER: richie_nau
        richie_mysql_replication_user: richie_replication

        # To fix Richie MySQL authentication
        # use legacy authentication plugin to fix: 
        # django.db.utils.OperationalError: (2059, "Authentication plugin 'caching_sha2_password' 
        # cannot be loaded: /usr/lib/x86_64-linux-gnu/mariadb18/plugin/caching_sha2_password.so: 
        # cannot open shared object file: No such file or directory
        mysql_docker_command_extra: --default-authentication-plugin=mysql_native_password

        # Base configuration for the MySQL instance
        mysql_docker_image: docker.io/mysql:8.0.29
        mysql_docker_replication: true
        mysql_docker_hostname: "{{ richie_mysql_docker_hostname }}"
        mysql_docker_port: "{{ richie_mysql_docker_port }}"
        mysql_docker_instance_name: richie_mysql
        mysql_docker_primary: "{{ richie_mysql_docker_primary }}"
        mysql_root_password: "{{ richie_MYSQL_ROOT_PASSWORD }}"
        mysql_server_id: "{{ groups.richie_mysql_docker_servers.index(inventory_hostname) +1 }}"

        mysql_health_check_user: "{{ richie_mysql_health_check_user }}"
        mysql_health_check_pass: "{{ richie_mysql_health_check_pass | default('') }}"

        # Advanced configuration that assumes that the first server is the primary replica.
        mysql_replication_login_password: "{{ richie_MYSQL_ROOT_PASSWORD }}"
        mysql_replication_master_server_ip: "{{ hostvars[groups.richie_mysql_docker_servers[0]].ansible_host }}"
        mysql_replication_master_port: "{{ hostvars[groups.richie_mysql_docker_servers[0]].richie_mysql_docker_port }}"
        mysql_replication_user: "{{ richie_mysql_replication_user }}"
        mysql_replication_password: "{{ richie_mysql_replication_password }}"
        mysql_replication_init_databases: [ "{{ richie_nau_MYSQL_DATABASE }}" ]
        mysql_replication_additional_databases:
          - "{{ richie_nau_MYSQL_DATABASE | default(None) }}"
        mysql_replication_additional_users:
          # one per site
          - {
              db: "{{ richie_nau_MYSQL_DATABASE | default(None) }}",
              user: "{{ richie_nau_MYSQL_USER | default(None) }}",
              pass: "{{ richie_nau_app_richie_MYSQL_PASSWORD | default(None) }}",
              host: "%"
            }
          - {
              db: "*",
              user: "{{ openark_orchestrator_client_user | default(None) }}",
              pass: "{{ openark_orchestrator_client_password | default(None) }}",
              host: "{{ openark_orchestrator_client_mysql_host }}",
              # ndbinfo.processes:SELECT
              priv: "*.*:SUPER,PROCESS,REPLICATION SLAVE,REPLICATION CLIENT,RELOAD/meta.*:SELECT/performance_schema.replication_group_members:SELECT"
            }
          - {
              db: "*",
              user: "{{ richie_mysql_health_check_user }}",
              host: "{{ richie_mysql_health_check_host | default('%') }}",
            }
        mysql_docker_configurations_dict:
          # The size in bytes of the buffer pool, the memory area where InnoDB caches table and index data.
          # Defaults to 128MB
          innodb_buffer_pool_size: "{{ richie_mysql_innodb_buffer_pool_size | default('1G') }}"
      when: richie_mysql_deploy | default(false) | bool

    - name: Restore keepalived priority
      import_role:
        name: keepalived
      vars:
        keepalived_priority_override: ""
      tags: keepalived
      when: ( richie_mysql_deploy | default(false) | bool ) and richie_mysql_keepalived_vrrp_instances is defined and ( groups['richie_mysql_docker_servers'] | length ) > 1


- name: Deploy financial manager mysql docker servers
  hosts: financial_manager_mysql_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: financial_manager_mysql_deploy | default(false) | bool

    - import_role:
        name: snmpd
      tags: snmpd
      when: financial_manager_mysql_deploy | default(false) | bool
    - import_role:
        name: snmpd_docker
      tags: snmpd
      when: financial_manager_mysql_deploy | default(false) | bool

    - name: Deploy financial manager mysql
      import_role:
        name: mysql_docker_deploy
      vars:
        # To fix Financial Manager MySQL authentication
        # use legacy authentication plugin to fix: 
        # django.db.utils.OperationalError: (2059, "Authentication plugin 'caching_sha2_password' 
        # cannot be loaded: /usr/lib/x86_64-linux-gnu/mariadb18/plugin/caching_sha2_password.so: 
        # cannot open shared object file: No such file or directory
        mysql_docker_command_extra: --default-authentication-plugin=mysql_native_password

        # Base configuration for the MySQL instance
        mysql_docker_image: docker.io/mysql:8.0.29
        mysql_major_release_number: 8
        mysql_docker_hostname: "{{ financial_manager_mysql_docker_hostname | default('nau_financial_manager_mysql') }}"
        mysql_docker_port: "{{ financial_manager_mysql_docker_port | default(3306) }}"
        mysql_docker_instance_name: financial_manager_mysql
        mysql_docker_primary: True
        mysql_root_password: "{{ financial_manager_mysql_root_password }}"
        mysql_server_id: "{{ groups.financial_manager_mysql_docker_servers.index(inventory_hostname) +1 }}"

        mysql_health_check_user: "{{ financial_manager_mysql_health_check_user | default('health_check_user') }}"
        mysql_health_check_pass: "{{ financial_manager_mysql_health_check_pass | default('') }}"

        # Advanced configuration that assumes that the first server is the primary replica.
        mysql_replication_login_password: "{{ financial_manager_mysql_root_password }}"
        # mysql_replication_master_server_ip: "{{ hostvars[groups.financial_manager_mysql_docker_servers[0]].ansible_host }}"
        # mysql_replication_master_port: "{{ hostvars[groups.financial_manager_mysql_docker_servers[0]].financial_manager_mysql_docker_port }}"
        # mysql_replication_user: "{{ financial_manager_mysql_replication_user }}"
        # mysql_replication_password: "{{ financial_manager_mysql_replication_password }}"
        mysql_replication_init_databases: [ "{{ financial_manager_mysql_database }}" ]
        mysql_replication_additional_databases:
          - "{{ financial_manager_mysql_database | default(None) }}"
        mysql_replication_additional_users:
          # one per site
          - {
              db: "{{ financial_manager_mysql_database | default(None) }}",
              user: "{{ financial_manager_mysql_user | default(None) }}",
              pass: "{{ financial_manager_mysql_password | default(None) }}",
              host: "%"
            }
          - {
              db: "*",
              user: "{{ financial_manager_mysql_health_check_user | default('health_check_user') }}",
              host: "{{ financial_manager_mysql_health_check_host | default('%') }}",
            }
      when: financial_manager_mysql_deploy | default(false) | bool

- name: Deploy financial manager servers
  hosts: financial_manager_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Start rolling deploy - block load balancer connections
      import_role:
        name: rolling_deploy
      vars:
        rolling_deploy_docker: true
        rolling_deploy_starting: true
      when: ( groups['financial_manager_docker_servers'] | length ) > 1 and ( financial_manager_docker_servers | default(false) | bool )

    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: financial_manager_deploy | default(false) | bool

    - import_role:
        name: snmpd
      tags: snmpd
      when: financial_manager_deploy | default(false) | bool
    - import_role:
        name: snmpd_docker
      tags: snmpd
      when: financial_manager_deploy | default(false) | bool

    - name: Deploy financial manager app
      import_role:
        name: financial_manager_docker_deploy
      when: financial_manager_deploy | default(false) | bool
      vars:
        # MySQL DB
        financial_manager_mysql_docker_hostname: "{{ hostvars[groups['financial_manager_mysql_docker_servers'][0]].ansible_host }}"
        financial_manager_mysql_docker_port: "{{ hostvars[groups['financial_manager_mysql_docker_servers'][0]].financial_manager_mysql_docker_port }}"
        financial_manager_mysql_database: "{{ hostvars[groups['financial_manager_mysql_docker_servers'][0]].financial_manager_mysql_database }}"
        financial_manager_mysql_user: "{{ hostvars[groups['financial_manager_mysql_docker_servers'][0]].financial_manager_mysql_user }}"
        financial_manager_mysql_password: "{{ hostvars[groups['financial_manager_mysql_docker_servers'][0]].financial_manager_mysql_password }}"
        financial_manager_mysql_root_password: "{{ hostvars[groups['financial_manager_mysql_docker_servers'][0]].financial_manager_mysql_root_password }}"

        # Django Cache
        financial_manager_caches_default_redis_host: "{{ hostvars[groups['redis_docker_servers'][0]].redis_virtual_ipv4 }}"
        financial_manager_caches_default_redis_port: "{{ hostvars[groups['redis_docker_servers'][0]].redis_docker_port }}"
        financial_manager_caches_default_redis_db:   "8"

        # Celery broker
        financial_manager_celery_broker_redis_host: "{{ hostvars[groups['redis_docker_servers'][0]].ansible_host }}"
        financial_manager_celery_broker_redis_port: "{{ hostvars[groups['redis_docker_servers'][0]].redis_docker_port }}"
        financial_manager_celery_broker_redis_db:   "9"

    - name: End rolling deploy - open load balancer connections
      import_role:
        name: rolling_deploy
      vars:
        rolling_deploy_docker: true
        rolling_deploy_starting: false
      when: ( groups['financial_manager_docker_servers'] | length ) > 1 and ( financial_manager_docker_servers | default(false) | bool )

- name: Deploy nau analytics mysql docker servers
  hosts: nau_analytics_mysql_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: nau_analytics_mysql_deploy | default(false) | bool

    - import_role:
        name: snmpd
      tags: snmpd
      when: nau_analytics_mysql_deploy | default(false) | bool
    - import_role:
        name: snmpd_docker
      tags: snmpd
      when: nau_analytics_mysql_deploy | default(false) | bool

    - name: Deploy nau analytics mysql
      import_role:
        name: mysql_docker_deploy
      vars:
        # To fix nau analytics MySQL authentication
        # use legacy authentication plugin to fix: 
        # django.db.utils.OperationalError: (2059, "Authentication plugin 'caching_sha2_password' 
        # cannot be loaded: /usr/lib/x86_64-linux-gnu/mariadb18/plugin/caching_sha2_password.so: 
        # cannot open shared object file: No such file or directory
        mysql_docker_command_extra: --default-authentication-plugin=mysql_native_password

        # Base configuration for the MySQL instance
        mysql_docker_image: docker.io/mysql:8.0.29
        mysql_major_release_number: 8
        mysql_docker_hostname: "{{ nau_analytics_mysql_docker_hostname | default(' nau_analytics_mysql') }}"
        mysql_docker_port: "{{ nau_analytics_mysql_docker_port | default(3306) }}"
        mysql_docker_instance_name: nau_analytics_mysql
        mysql_docker_primary: True
        mysql_root_password: "{{ nau_analytics_mysql_root_password }}"
        mysql_server_id: "{{ groups.nau_analytics_mysql_docker_servers.index(inventory_hostname) +1 }}"

        mysql_health_check_user: "{{ nau_analytics_mysql_health_check_user | default('health_check_user') }}"
        mysql_health_check_pass: "{{ nau_analytics_mysql_health_check_pass | default('') }}"

        # Advanced configuration that assumes that the first server is the primary replica.
        mysql_replication_login_password: "{{ nau_analytics_mysql_root_password }}"
        # mysql_replication_master_server_ip: "{{ hostvars[groups.nau_analytics_mysql_docker_servers[0]].ansible_host }}"
        # mysql_replication_master_port: "{{ hostvars[groups.nau_analytics_mysql_docker_servers[0]].nau_analytics_mysql_docker_port }}"
        # mysql_replication_user: "{{ nau_analytics_mysql_replication_user }}"
        # mysql_replication_password: "{{ nau_analytics_mysql_replication_password }}"
        mysql_replication_init_databases: [ "{{ nau_analytics_mysql_database }}" ]
        mysql_replication_additional_databases:
          - "{{ nau_analytics_mysql_database | default(None) }}"
        mysql_replication_additional_users:
          # one per site
          - {
              db: "{{ nau_analytics_mysql_database | default(None) }}",
              user: "{{ nau_analytics_mysql_user | default(None) }}",
              pass: "{{ nau_analytics_mysql_password | default(None) }}",
              host: "%"
            }
          - {
              db: "*",
              user: "{{ nau_analytics_mysql_health_check_user | default('health_check_user') }}",
              host: "{{ nau_analytics_mysql_health_check_host | default('%') }}",
            }
      when: nau_analytics_mysql_deploy | default(false) | bool

- name: Deploy to XtraDB database
  hosts: xtradb_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Lower keepalived priority to force VIP swap
      import_role:
        name: keepalived
      vars: 
        keepalived_priority_override: 1 # Lower priority so the VIPs can swap to other machine.
      tags: keepalived
      when: ( xtradb_deploy | default(false) | bool ) and xtradb_keepalived_vrrp_instances is defined and ( groups['xtradb_servers'] | length ) > 1
    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: xtradb_deploy | default(false) | bool
    - name: Install snmpd
      import_role:
        name: snmpd
      tags: snmpd
      when: xtradb_deploy | default(false) | bool
    - name: Install snmpd docker bridge
      import_role:
        name: snmpd_docker
      tags: snmpd
      when: xtradb_deploy | default(false) | bool
    - name: Deploy Percona XtraDB
      import_role:
        name: xtradb_docker_deploy
      vars:
        # To initialize the xtradb cluster, only 1st node, 1st time with:
        #   -e xtradb_cluster_initialization=true
        xtradb_cluster_hosts_tmp: "{% for host in groups['xtradb_servers'] %}{{ hostvars[host].ansible_host }}{{ ',' if not loop.last else '' }}{% endfor %}"
        xtradb_cluster_hosts: "{{ xtradb_cluster_hosts_tmp.split(',') | list }}"
        xtradb_certs_ca_key: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/ca-key.pem"
        xtradb_certs_ca: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/ca.pem"
        xtradb_certs_client_cert: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/client-cert.pem"
        xtradb_certs_client: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/client-key.pem"
        xtradb_certs_private_key: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/private_key.pem"
        xtradb_certs_public_key: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/public_key.pem"
        xtradb_certs_server_cert: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/server-cert.pem"
        xtradb_certs_server_key: "{{ COMMON_PATH_CUSTOM_FILES }}/xtradb/cert/server-key.pem"
        # Prefer the last node in the cluster to be used for state transfer from the cluster.
        # If the list contains a trailing comma, the remaining nodes in the cluster
        # will also be considered if the nodes from the list are not available.
        xtradb_docker_wsrep_sst_donor: "{{ groups['xtradb_servers'][-1] }},"
      when: xtradb_deploy | default(false) | bool
    - name: Lower keepalived priority to force VIP swap
      import_role:
        name: keepalived
      vars:
        keepalived_priority_override: ""
      tags: keepalived
      when: ( xtradb_deploy | default(false) | bool ) and xtradb_keepalived_vrrp_instances is defined and ( groups['xtradb_servers'] | length ) > 1


- name: Deploy to ClickHouse database
  hosts: clickhouse_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Lower keepalived priority to force VIP swap
      import_role:
        name: keepalived
      vars: 
        keepalived_priority_override: 1 # Lower priority so the VIPs can swap to other machine.
      tags: keepalived
      when: ( clickhouse_deploy | default(false) | bool ) and clickhouse_keepalived_vrrp_instances is defined

    - name: Install or upgrade docker daemon
      import_role:
        name: geerlingguy.docker
      when: clickhouse_deploy | default(false) | bool
    - name: Install snmpd
      import_role:
        name: snmpd
      tags: snmpd
      when: clickhouse_deploy | default(false) | bool
    - name: Install snmpd docker bridge
      import_role:
        name: snmpd_docker
      tags: snmpd
      when: clickhouse_deploy | default(false) | bool
    - name: Deploy ClickHouse
      import_role:
        name: clickhouse_docker_deploy
      when: clickhouse_deploy | default(false) | bool
      vars:
        clickhouse_cluster_hosts: "{{ groups['clickhouse_servers'] }}"
    - name: Lower keepalived priority to force VIP swap
      import_role:
        name: keepalived
      vars:
        keepalived_priority_override: ""
      tags: keepalived
      when: ( clickhouse_deploy | default(false) | bool ) and clickhouse_keepalived_vrrp_instances is defined


- name: Deploy to Kubernetes nodes
  hosts: kubernetes_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - import_role:
        name: snmpd
      tags: snmpd
      when: kubernetes_deploy | default(false) | bool


# Should be the last, because it depends on the docker, and the business projects are the ones that install docker
- name: Deploy observability docker servers
  hosts: observability_docker_servers
  serial: "{{ serial_number | default(1) }}" # deploy in sequence
  become: True
  gather_facts: True
  tasks:
    - name: Deploy observability as docker compose file
      import_role:
        name: observability_docker_deploy
      vars:
        observability_docker_deploy_run: "{{ observability_docker_deploy | default(false) }}"
